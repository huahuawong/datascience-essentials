{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "### 1. What is PCA?\n",
    "PCA is a dimensionality reduction technique that is widely used. Dimension reduction? What does that even mean? Let's say you have a dataset with 500 features, which is too many, so ideally we want to find the important features instead of using the entire set of features. These features would still contain most of the information.\n",
    "\n",
    "Reducing the number of features of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n",
    "\n",
    "### 2. Is it supervised or unsupervised?\n",
    "Unsupervised. No labels or targets/ \n",
    "\n",
    "### 3. Step by step for PCA?\n",
    "i. Standardization - Similar to k-means, normalize the data to make sure that the values are scaled. \n",
    "\n",
    "ii. Compute covariance matrix\n",
    "\n",
    "iii. Compute eigenvectors and eigenvalues to identify the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonCPU]",
   "language": "python",
   "name": "conda-env-PythonCPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
